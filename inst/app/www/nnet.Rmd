---
title: "Neural net analysis, automated"
author: "Generated by RTutor.ai"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_sections: true
    code_folding: hide
    toc: true
params:
  df: NULL
  target: NULL
  predictors: NULL
  size: NULL
  positive_class_variable: NULL
  negative_class_variable: NULL
  date:
    label: "Date: "
    value: !r Sys.Date()
  printcode:
    label: "Display Code"
    value: TRUE
    input: checkbox
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(nnet)
library(caret)
library(pROC)
library(ggplot2)
df <- params$df
target <- params$target
predictors <- params$predictors
size <- params$size
positive_class_label <- params$positive_class_variable
negative_class_label <- params$negative_class_variable
```


# Data Summary
This report presents the results of a neural network analysis using the nnet package. We will evaluate models for
both classification and regression tasks, depending on the nature of the target variable.

```{r data-summary}
cat("Head of dataframe:")
head(df) # Display the first few rows of the dataframe
cat("Summary of dataframe:")
summary(df) # Summary statistics
```


# Data Pre-process

```{r data-prep}
# Extract the predictor variables
predictors <- setdiff(predictors, target)

# Ensure data types are correct
size <- as.numeric(size)

# Display the selected variables
cat("Target Variable:", target, "\n")
cat("Predictor Variables:", paste(predictors, collapse = ", "), "\n")
```

If a numeric variable has just a few unique values, it might make more sense to convert it into categorical variable.
If the total number of unique values is less than 5% of the total rows, convert.
```{r convert-factor}
# Iterate over each column of the data frame
for (i in seq_along(df)) {
  # Check if the column is numeric and has less than 5 unique values
  if (is.numeric(df[[i]]) && length(unique(df[[i]])) / nrow(df) < 0.05 && length(unique(df[[i]])) < 13) {
    # Convert the column to a factor
    df[[i]] <- as.factor(df[[i]])
    cat("\nColumn ", colnames(df)[i], "was converted to a factor.")
  }
}
```

If a non-numeric variable has too many unique values, it might be names or IDs that is not useful in analysis. 
```{r convert-id}
cutoff <- 0.8
# Initialize a vector to store the names of columns to be removed
cols_to_remove <- c()

# Loop through each column
for (col_name in names(df)) {
  # Check if column is non-numeric and has unique values > 80% of total rows
  if (!is.numeric(df[[col_name]]) && length(unique(df[[col_name]])) > cutoff * nrow(df)) {
    cols_to_remove <- c(cols_to_remove, col_name)
    cat("\nColumn", cols_to_remove, " was excluded from analysis.")
  }
}

# Remove the identified columns
df <- df %>% select(-one_of(cols_to_remove))
```


# Neural Network for Classification
If the target variable is categorical (a factor) then we will perform analysis for classification.

```{r nnet-classification}
# Check if target variable is a factor
if (is.factor(df[[target]]) | is.character(df[[target]])) {
  # Create a classification neural network using the nnet package
  # Set a seed for reproducibility
  set.seed(123)
  df[[target]] <- as.factor(df[[target]])

  # Split the data into training and testing sets
  split <- createDataPartition(df[[target]], p = 0.7, list = FALSE)
  train_data <- df[split, ]
  test_data <- df[-split, ]

  # Define the formula for the model
  formula <- as.formula(paste(target, "~", paste(predictors, collapse = "+")))

  # Fit the model using nnet
  cat("Neural Network Summary:\n\n")
  nn_model <- nnet(formula, data = train_data, size = size, linout = FALSE)


  if (nlevels(df[[target]]) == 2) { # Binary Classification

    # Ensure positive class is the second level
    if(levels(test_data[[target]])[2] != positive_class_label) {
        test_data[[target]] <- relevel(test_data[[target]],
                                       ref = setdiff(levels(test_data[[target]]), positive_class_label)
                                       )
    }

    # Predict probabilities for the positive class based on the model
    raw_pred <- predict(nn_model, newdata = test_data, type = "raw")  # raw predictions
    pred_prob <- 1 / (1 + exp(-raw_pred))  # transform to probabilities

    # AUC-ROC for testing data
    roc_curve <- roc(test_data[[target]], pred_prob)
    auc_value <- auc(roc_curve)
    plot(roc_curve, main = paste("ROC Curve ( AUC =", round(auc_value, 3), ")"))
    
    # Calculate optimal threshold using Youden's J statistic
    optimal_threshold <- coords(roc_curve, "best", ret = "threshold")

    # Convert probabilities to predicted class using the optimal threshold
    predict_class <- ifelse(pred_prob >= optimal_threshold$threshold[1], positive_class_label, negative_class_label)
    predicted_class <- factor(predict_class, levels = levels(test_data[[target]]))

    # Calculate confusion matrix
    conf_matrix <- confusionMatrix(predicted_class, test_data[[target]], positive = positive_class_label)
    
    # Extract precision, recall, and F1 score
    precision <- conf_matrix$byClass["Pos Pred Value"]
    recall <- conf_matrix$byClass["Sensitivity"]
    f1_score <- conf_matrix$byClass["F1"]
    
    # Print results
    cat("Model Performance Metrics for Positive Class:", positive_class_label, "\n\n")
    cat("Precision:", round(precision, 3), "\nWhen the model predicts the positive class, it's correct", 
        format(round(precision * 100, 1), nsmall=1),"% of the time.\n\n")
    cat("Recall:", round(recall, 3), "\nThe model correctly identifies", 
        format(round(recall * 100, 1), nsmall=1),"% of all actual positive instances.\n\n")
    cat("F1 Score:", round(f1_score, 3), "\nThe harmonic mean of precision and recall.\n\n")
    cat("AUC:", round(auc_value, 3), "\n")

  } else if (nlevels(df[[target]]) > 2) {   # Multi-class Classification

    # Plot the neural network model
    library(devtools)
    source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
    plot.nnet(nn_model)

    # Predict classes based on the model
    pred_class <- as.factor(predict(nn_model, newdata = test_data, type = "class"))
    pred_class <- factor(pred_class, levels = levels(df[[target]])) # Ensure their levels match

    # Analyze the model
    conf_matrix <- confusionMatrix(pred_class, test_data[[target]])

    # Print results
    cat("Multi-class Classification Model Analysis:\n\n")
    print(conf_matrix$overall["Accuracy"])
    print(conf_matrix$byClass[, c("Precision", "Recall", "F1")])

  } else {
    cat("The target variable does not have sufficient levels.\n")
  }

} else {
  print("Target variable is not categorical.")
}
```


# Neural Network for Regression
If the target variable is numeric then we will perform analysis for regression.

```{r nnet-regression}
# Check if target variable is numeric
# Neural Network for Regression
if (is.numeric(df[[target]]) | is.integer(df[[target]]) | is.double(df[[target]])) {
  # Set a seed for reproducibility
  set.seed(123)
  df[[target]] <- as.numeric(df[[target]])

  # Split the data into training and testing sets
  split <- createDataPartition(df[[target]], p = 0.7, list = FALSE)
  train_data <- df[split, ]
  test_data <- df[-split, ]

  # Define the formula for the model
  formula <- as.formula(paste(target, "~", paste(predictors, collapse = "+")))

  # Train the neural network for regression
  cat("Neural Network Summary:\n")
  nn_model_regression <- nnet(formula, data = train_data, size = size, maxit = 1000, linout = TRUE)

  # Predict on the test set
  predictions <- predict(nn_model_regression, newdata = test_data)

  # Calculate the Root Mean Squared Error (RMSE)
  rmse <- RMSE(predictions, test_data[[target]])
  cat("\nRoot Mean Squared Error (RMSE):", rmse, "\n")

  # Create a dataframe for plotting
  plot_df <- data.frame(Actual = test_data[[target]], Predicted = predictions)

  # Plot predicted vs. actual
  ggplot(plot_df, aes(x = Actual, y = Predicted)) +
    geom_point() +
    geom_abline(slope = 1, intercept = 0, color = "blue", linetype = "dashed") +
    labs(title = "Predicted vs Actual for Regression", x = "Actual", y = "Predicted") +
    theme_minimal()
} else {
  print("Target variable is not numeric.")
}
```
